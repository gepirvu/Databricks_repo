{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa5a86f-2130-4ea2-930f-e1ac54205940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Enable inline plotting for Databricks\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a308f72c-a8f7-4b65-bd21-b178f2344618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"abfss://raw@cloudinfrastg.dfs.core.windows.net/00_data_source/\"\n",
    "data = \"housing.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "housing = spark.read.csv(source + data, header=True, inferSchema=True)   \n",
    "housing.display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7da077-1a5c-4b82-831d-af56429621ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_pd = housing.toPandas ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1724c3f7-2172-42f8-9452-59dd53ff3dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b03ed6f-8500-4d68-bf64-157979700469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the data\n",
    "# One-hot encode the categorical \"ocean_proximity\" feature\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=\"if_binary\")  # Avoid sparse matrix and handle binary automatically\n",
    "encoded_features = encoder.fit_transform(housing_pd[[\"ocean_proximity\"]])\n",
    "encoded_feature_names = encoder.get_feature_names_out([\"ocean_proximity\"])\n",
    "\n",
    "# Create a DataFrame for encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Add encoded features to the original DataFrame and drop the original column\n",
    "housing_pd = pd.concat([housing_pd, encoded_df], axis=1).drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = housing_pd.drop(columns=[\"median_house_value\"])\n",
    "y = housing_pd[\"median_house_value\"]\n",
    "\n",
    "# Display the dataset\n",
    "display(X)\n",
    "\n",
    "# Step 2: Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=10, )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbff6a4f-f160-4e3a-8581-b501435b696e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b195ab1-bc9d-4564-95f2-acd0823d51a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Use SHAP to analyze feature importance\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b2778b-4517-44c4-a6e1-f2130a70c5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize SHAP values\n",
    "# Global importance summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f032bf-9b91-417a-84b7-c2ae0c875b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Force plot for an individual prediction (e.g., the first test sample)\n",
    "import shap\n",
    "shap.initjs() \n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0], feature_names=X_test.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ea534f-cb5c-46b2-8289-7bd76d9d78b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "shap",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
