{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1ed5b6-b087-4f16-bf5a-5f87fc64198e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382c1c86-d587-466d-8147-ac2c7b4e3440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"abfss://raw@cloudinfrastg.dfs.core.windows.net/00_data_source/\"\n",
    "data = \"housing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7731a65-7915-4718-882f-f270a636a097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "housing = spark.read.csv(source + data, header=True, inferSchema=True)   \n",
    "housing.display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14d20b9-6408-4a5e-9cd2-bed562bfda20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_pd = housing.toPandas ()\n",
    "housing_pd['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ded45b-2f68-4930-9954-378d89186e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop categorical feature for now (ocean_proximity)\n",
    "housing_pd = housing_pd.drop(columns=['ocean_proximity'])\n",
    "\n",
    "# Handle missing values\n",
    "housing_pd = housing_pd.fillna(housing_pd.median())\n",
    "\n",
    "# Split dataset into features and target\n",
    "X = housing_pd.drop(columns=['median_house_value'])\n",
    "y = housing_pd['median_house_value']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1aa882-e1bf-4695-9824-b25b603a08e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Gradient Boosted Tree Model (Using HistGradientBoostingRegressor)\n",
    "gbm_model = HistGradientBoostingRegressor(max_iter=100, learning_rate=0.1, random_state=42)\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gbm = gbm_model.predict(X_test)\n",
    "mae_gbm = mean_absolute_error(y_test, y_pred_gbm)\n",
    "print(f'Gradient Boosted Model MAE: ${mae_gbm:,.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b28916-9c02-4557-a044-4a2c527512f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Random Forest Regressor (Replacing LSTM)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f'Random Forest Model MAE: ${mae_rf:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f83c06-bc51-479c-be3d-0f011661b7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Why Use PCA in Our Housing Dataset?\n",
    "Our dataset has multiple numerical features, such as:\n",
    "\n",
    "Longitude, Latitude → Geographical location\n",
    "Housing Median Age → Age of the neighborhood\n",
    "Total Rooms, Bedrooms, Population, Households → Structural information\n",
    "Median Income → Economic factor\n",
    "Instead of using all these features, PCA finds a smaller number of new features (principal components) that capture most of the important variations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e32ee8-e8e0-4ef2-b702-629760ff7553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. PCA-based Dimensionality Reduction\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "pca_regressor = LinearRegression()\n",
    "pca_regressor.fit(X_train_pca, y_train)\n",
    "y_pred_pca = pca_regressor.predict(X_test_pca)\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "print(f'PCA Model MAE: ${mae_pca:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bb22fb-091e-4d11-9cf7-8cc000295b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stacking Ensemble\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('gbm', gbm_model),\n",
    "        ('rf', rf_model),\n",
    "        ('pca', pca_regressor)\n",
    "    ],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "print(f'Stacking Ensemble MAE: ${mae_stack:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "285cd7d0-0842-4186-9359-b60dad22e7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explanation of Choice\n",
    "We chose Stacking as the ensemble method since it leverages multiple diverse models\n",
    "(GBM, Random Forest, and PCA) and uses a meta-learner (Linear Regression) to combine predictions.\n",
    "This enhances generalization and ensures better performance compared to individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6429c2-ceb8-4b9f-b082-da24e1e06d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot Actual vs Predicted Prices\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_stack, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='dashed')\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Actual vs Predicted House Prices (Stacking Ensemble)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ensemble model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
